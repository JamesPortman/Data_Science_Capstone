iClairvoyance
========================================================
author: James Portman
css: custom.css
date: October 4, 2016
autosize: TRUE
transition: rotate

<br>
This pitch is for the next generation smart phone text prediction.
<p>
Part of the Capstone project for the Data Science Certification offered through John Hopkins University.


What does iClairvoyance do?
========================================================

It predicts the next possible word in a sentence given the following constraints:

- Simulate working within smart phone performance limits.
- Load and provide predictions quickly.
- Provide a prediction for every known phrase entered.

<b>Data</b><br>
Almost 10 million tweets, blogs and news sources were randomly sampled into a corpus of 15,000 sentences. The corpus was cleaned by removing white space, punctuation, numbers, and then converted to lowercase.

How does the predictive model work?
========================================================

STEP 1: <b>Create an n-grams database</b><br>
N-grams are a contiguous sequence of n items from a given sequence of text or speech. A tokenizer was used to create 1-gram, 2-gram, 3-grams, and 4-grams from the Corpus. The N-grams are then sorted by frequency and stored in data files to be quickly loaded by the Shiny App.

STEP 2: <b>Implement a high-speed prediction alorithm</b><br>
A back-off model was used to estimate the conditional probability of a word given its history in an N-gram. Under certain conditions, the estimation is provided by "backing-off" to models with smaller histories.


Shiny App in Action
========================================================

The App can be seen at: https://portman.shinyapps.io/iClairvoyance/ <br>

![Instructions](Final_Pitch_screen_snapshot.png)

The app takes a phrase in a text box as input and outputs the top prediction for the next word. 

Generation Two
========================================================
Given your funding, the following improvements can be made:
- Instead of using Katz's back-off model use models such as Good-Turing discounting and Kneser-Ney smoothing.
- Use skip-grams, where words need not be consecutive in the phrase, to overcome the data sparsity problem.
- Create a hash table to speed up the app's response time.
- Allow previously typed user phrases to be added to the vocabulary and ranking.<br>

<b>Open source GitHub repository</b> https://github.com/JamesPortman/Data_Science_Capstone
