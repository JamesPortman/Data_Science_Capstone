Word Predictor App
========================================================
author: James Portman
date: October 3, 2016
autosize: false
transition: fade

<br>
This presentation describes the algorithm used in a Word Predictor App. The App is published as a Shiny app. A link and instructions for using the App is be detailed on a subsequent slide.
<p>
Part of the Capstone project for the Data Science Certification offered through John Hopkins University.


Goal and Data source
========================================================

The goal is to predict the next possible word based on previous words in the sentence. Constraints are:

- Work within Shiny App memory and performance limits.
- Load quickly and provide predictions quickly.
- Provide a prediction (or Unknown) for every phrase entered.

<b>Data</b><br>
SwiftKey provided three files: Twitter tweets, Blog posts, and News entries. These files were randomly sampled and combined into a corpus of 15,000 sentences. The corpus was cleaned by removing white space, punctuation, numbers, and then converted to lowercase.

How the predictive model works
========================================================

STEP 1: CREATING N-GRAMS<br>
An n-gram is a contiguous sequence of n items from a given sequence of text or speech. A tokenizer was used to create 1-gram, 2-gram, 3-grams, and 4-grams from the Corpus. The n-grams are then sorted by frequency and stored in data files to be quickly loaded by the Shiny App.

STEP 2: PREDICTION<br>
A back-off model was used to estimate the conditional probability of a word given its history in an n-gram. Under certain conditions, the estimation is provided by "backing-off" to models with smaller histories.


Shiny App in Action
========================================================

The App can be seen at: https://portman.shinyapps.io/WordPredictorApp/ <br>

![Instructions](screen_snapshot.png)

The app takes as input a phrase in a text box input and outputs the top predictions for the next word. The ranked score shows a measure of the relatively frequency.<br>

Prediction Improvements
========================================================
- Create a hash table to speed up the app's response time.
- Instead of using Katz's back-off model try models such as Good-Turing discounting and Kneser-Ney smoothing.
- Use skip-grams, where words need not be consecutive in the phrase, to overcome the data sparsity problem.
- Allow previously typed user phrases to be added to the vocabulary and ranking.<br>


<b>GitHub repository</b> https://github.com/JamesPortman/Data_Science_Capstone
