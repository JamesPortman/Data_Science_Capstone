---
title: "Data Science Capstone Milestone Report"
author: "James Portman"
date: "September 4, 2016"
output: html_document
---

## Instructions
The goal of this Milestone project is to demonstrate proficiency working with the data and progress with the prediction algorithm. It will show the exploratory analysis and the goals for the eventual app and algorithm. 

This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set.<br>
The motivation for this Milestone project is to:<br>
* Demonstrate that you've downloaded the data and have successfully loaded it in. <br>
* Create a basic report of summary statistics about the data sets. <br>
* Report any interesting findings that you amassed so far. <br>
* Get feedback on your plans for creating a prediction algorithm and Shiny app.<br>

## Steps
1.0 Download and unzip data.<br>
2.0 Load data.<br>
3.0 Create Summary statistics.<br>
3.1 Sample the data.<br>
3.2 Explore sampled data.<br>
4.0 Combine Samples into a Corpus.<br>
4.1 Explore Word frequency in Corpus.<br>
5.0 Planning Next steps.<br>

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)

# libraries required
library(stringi)
library(ggplot2)
library(gridExtra)
library(tm)
library(quanteda)

setwd("/Users/admin/Capstone/")
set.seed(1969)
```

### 1.0 Download and unzip data
Original data source: <br>
http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
```{r download_data}
# download.file(url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
#               destfile = "Coursera-SwiftKey.zip", method = "curl")

#  unzip('Coursera-SwiftKey.zip') 
```
  
The unzipped files include the following:<br>
* en_US.blogs.txt <br>
* en_US.news.txt <br>
* en_US.twitter.txt <br>

### 2.0 Load data

```{r load_data}
# Read in Blog file
blogs <- readLines(con <- file("./en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con) # Close the connection to the file.

# Read in News file
news <- readLines(con <- file("./en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con) # Close the connection to the file.

# Read in Twitter file
tweets <- readLines(con <- file("./en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con) # Close the connection to the file.
```

### 3.0 Create Summary statistics
```{r summary_lines}
number_of_lines <- c(length(blogs), length(news), length(tweets))
number_of_lines <- data.frame(number_of_lines)
number_of_lines$names <- c("Blogs","News","Twitter")

# Lines data table
number_of_lines

# Lines Plot
ggplot(number_of_lines,aes(x=names,y=number_of_lines)) + geom_bar(stat='identity', fill="green", color='green') + xlab('Source') + ylab('# of Lines') + ggtitle('Lines per Source')
```

```{r summary_words}
blogs.words <- sum(sapply(gregexpr("\\W+", blogs), length) + 1)
news.words <- sum(sapply(gregexpr("\\W+", news), length) + 1)
tweets.words <- sum(sapply(gregexpr("\\W+", tweets), length) + 1)
number_of_words <- c(blogs.words, news.words, tweets.words)
number_of_words <- data.frame(number_of_words)
number_of_words$names <- c("Blogs","News","Twitter")

# Words data table
number_of_words

# Word Plot
ggplot(number_of_words,aes(x=names, y=number_of_words)) + geom_bar(stat='identity', fill="blue", color='blue') + xlab('Source') + ylab('# of words') + ggtitle('Words per Source')
```

### 3.1 Sample the data
```{r sample}
sampleSize <- 10000
sampleBlogs <- sample(blogs, sampleSize)
sampleNews <- sample(news, sampleSize)
sampleTweets <- sample(tweets, sampleSize)

```

### 3.2 Explore sampled data
```{r explore_data}
head(sampleBlogs, 3)

head(sampleNews, 3)

head(sampleTweets, 3)
```

### 4.0 Combine Samples into a Corpus
```{r message=FALSE}
corpus1 <- corpus(sampleBlogs)
corpus2 <- corpus(sampleNews)
corpus3 <- corpus(sampleTweets)
corpus_sum <- corpus1 + corpus2 + corpus3

# Create Document Frequency Matrix
theDFM <- dfm(corpus_sum, verbose = TRUE, toLower = TRUE, removeTwitter = TRUE, 
             removeNumbers = TRUE, ignoredFeatures = stopwords("english"))
```

### 4.1 Explore Word frequency in Corpus
```{r message=FALSE}
barplot(topfeatures(theDFM, 10), main="Top 10 Words in Corpus", col="orange", ylab="Frequency")
plot(theDFM, min.freq = 500, random.order = FALSE) # Word Cloud.
```

## 5.0 Planning Next steps
* Develop a predictive model that can predict the current word based on the first few letters.<br>
* Develop a predictive model that suggest one or more next words.<br>
* Build a validation framework to test model performance.<br>
* Deploy model as a Shiny App.<br>
